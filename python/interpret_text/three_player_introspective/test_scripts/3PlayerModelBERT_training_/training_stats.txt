INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.2600, test acc: 0.2600
INFO:root:train acc: 0.4000, test acc: 0.2600
INFO:root:train acc: 0.3000, test acc: 0.2600
INFO:root:train acc: 0.3400, test acc: 0.2600
INFO:root:train acc: 0.4600, test acc: 0.3600
INFO:root:train acc: 0.5400, test acc: 0.4200
INFO:root:train acc: 0.5600, test acc: 0.4200
INFO:root:train acc: 0.5800, test acc: 0.4000
INFO:root:train acc: 0.5600, test acc: 0.3600
INFO:root:train acc: 0.5600, test acc: 0.3800
INFO:root:test acc: 0.2600 test anti acc: 0.2600
INFO:root:test sparsity: 0.3261 test continuity: 0.1654
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] of [##t] - described as the anti ##dote [to] [american] [pie] [-] [type] [sex] [comedies] [,] [it] [actually] has a [bundle] [in] [common] [with] [them] [,] [as] [the] [film] [diffuse] [##s] [every] [opportunity] [for] a [breakthrough] [[SEP]] 
INFO:root:test acc: 0.4000 test anti acc: 0.2600
INFO:root:test sparsity: 0.3133 test continuity: 0.1637
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] not a cozy or ing ##rat ##iating work , [but] [it] ['] [s] challenging , [sometimes] [clever] [,] and always [interesting] [,] [and] [those] [are] [reasons] [enough] [to] [see] [it] [.] [[SEP]] 
INFO:root:test acc: 0.4200 test anti acc: 0.3000
INFO:root:test sparsity: 0.3002 test continuity: 0.1588
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] the low - [budget] [full] [frontal] [was] [one] of the [year] ['] [s] mu [##rk] [##iest] [,] intentionally [obscure] [and] [self] [-] [ind] ##ul ##gent [pictures] [,] [and] [solar] [##is] [is] [its] [big] [-] [budget] [brother] [.] [SEP] 
INFO:root:test acc: 0.4200 test anti acc: 0.4000
INFO:root:test sparsity: 0.2808 test continuity: 0.1522
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] of ##t - described as the anti ##dote to [american] [pie] [-] [type] [sex] [comedies] [,] [it] actually has a [bundle] [in] [common] [with] [them] [,] [as] [the] [film] [diffuse] [##s] [every] [opportunity] [for] a [breakthrough] [[SEP]] 
INFO:root:test acc: 0.3600 test anti acc: 0.3800
INFO:root:test sparsity: 0.2712 test continuity: 0.1537
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[CLS] it ['] s a pleasure to [see] se ##in [##feld] [grip] [##ing] [about] the bi [##z] [with] [buddies] [chris] [rock] [,] [garry] shan ##dling and colin [quinn] [.] [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.4400
INFO:root:test sparsity: 0.2607 test continuity: 0.1541
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] a go ##b of drive ##l so sick ##ly [sweet] , even the eager [consumers] of [moore] ['] [s] [paste] ##urized di ##tti ##es will re ##tch [it] [up] [like] [ran] [##ci] [##d] [cr] [##eme] [br] [##ule] ##e . [SEP] 
INFO:root:test acc: 0.2800 test anti acc: 0.3800
INFO:root:test sparsity: 0.2464 test continuity: 0.1517
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] if [your] senses have n ' t been dull ##ed by slash [##er] films and gore [##fest] [##s] , [if] [you] ['] re a [con] [##no] [##isse] [##ur] of [psychological] [horror] [,] [this] [is] [your] [ticket] [.] [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.3200
INFO:root:test sparsity: 0.2374 test continuity: 0.1417
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] cu ##lk ##in ex ##udes none of the charm or char ##ism ##a that might [keep] a [more] [general] [audience] [even] [vaguely] [interested] in his [brat] [##ty] [character] [.] [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.2800
INFO:root:test sparsity: 0.2268 test continuity: 0.1325
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] gangs of new york is an una ##pol ##oge ##tic mess , whose [only] [saving] [grace] [is] [that] [it] [ends] [by] [blowing] just about [everything] [up] [.] [SEP] 
INFO:root:test acc: 0.2400 test anti acc: 0.2800
INFO:root:test sparsity: 0.2162 test continuity: 0.1254
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] it ' s a pleasure to see se ##in [##feld] [grip] [##ing] about the bi [##z] [with] [buddies] [chris] [rock] [,] [garry] shan ##dling and colin quinn [.] [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.2400
INFO:root:test sparsity: 0.2091 test continuity: 0.1182
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the premise for this ke ##gger comedy probably [sounded] brilliant four six - packs and a pitcher of [margarita] [##s] in , [but] [the] [film] must [have] [been] [written] [.] [.] [.] [in] [the] [th] [##ral] [##l] of a vicious [hang] ##over . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.2800
INFO:root:test sparsity: 0.2010 test continuity: 0.1202
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] the passions aroused by the [disco] [##rd] between old and [new] cultures are set [against] the strange , [stark] [beauty] [of] the mid ##ea [##st] [desert] [,] [so] [loving] [##ly] and per [##ceptive] [##ly] [filmed] [that] [you] [can] [almost] [taste] [the] [des] [##ic] ##cated air . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.3000
INFO:root:test sparsity: 0.1938 test continuity: 0.1166
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] it ' s a pleasure to see se ##in [##feld] [grip] [##ing] about the bi [##z] [with] [buddies] [chris] [rock] [,] [garry] shan ##dling and colin quinn . [SEP] 
INFO:root:test acc: 0.3200 test anti acc: 0.3000
INFO:root:test sparsity: 0.1871 test continuity: 0.1137
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] though the violence is far [less] sad ##istic than usual , the [film] is typical mi [##ike] [:] [fast] [,] [furious] [and] [full] of [off] [-] [the] [-] [cuff] [imaginative] flourish ##es . [SEP] 
INFO:root:test acc: 0.3200 test anti acc: 0.3200
INFO:root:test sparsity: 0.1830 test continuity: 0.1090
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the whole thing plays out with the dr ##ows ##y he ##avi ##ness of synchronized swimmer [wearing] a [wool] wet ##suit . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.3200
INFO:root:test sparsity: 0.1759 test continuity: 0.1029
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] this is one of pol ##ans ##ki ' s [best] films . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.3000
INFO:root:test sparsity: 0.1746 test continuity: 0.1017
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the whole thing plays out with the dr ##ows ##y he ##avi ##ness of synchronized swimmer [wearing] a [wool] wet ##suit . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.3000
INFO:root:test sparsity: 0.1681 test continuity: 0.0945
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] it ' s a pleasure to see se ##in [##feld] [grip] [##ing] about the bi [##z] [with] [buddies] [chris] [rock] [,] [garry] shan ##dling and colin quinn . [SEP] 
INFO:root:test acc: 0.2800 test anti acc: 0.2800
INFO:root:test sparsity: 0.1642 test continuity: 0.0886
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the film would work much [better] as a video installation in a museum , [where] [viewers] would [be] [free] [to] [leave] . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.2800
INFO:root:test sparsity: 0.1584 test continuity: 0.0857
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] arnold ' s jump from little screen to big will leave frowns on more than a few [faces] . [SEP] 
INFO:root:test acc: 0.3200 test anti acc: 0.3000
INFO:root:test sparsity: 0.1508 test continuity: 0.0833
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the film would work much [better] as a video installation in a museum , [where] [viewers] would [be] [free] [to] leave . [SEP] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.3200, test acc: 0.2600
INFO:root:train acc: 0.4000, test acc: 0.2600
INFO:root:train acc: 0.3800, test acc: 0.2600
INFO:root:train acc: 0.3600, test acc: 0.2600
INFO:root:train acc: 0.3800, test acc: 0.2600
INFO:root:train acc: 0.4400, test acc: 0.4000
INFO:root:train acc: 0.5000, test acc: 0.4200
INFO:root:train acc: 0.5800, test acc: 0.4000
INFO:root:train acc: 0.6000, test acc: 0.4000
INFO:root:train acc: 0.5600, test acc: 0.4200
INFO:root:test acc: 0.3800 test anti acc: 0.3200
INFO:root:test sparsity: 0.3698 test continuity: 0.1883
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[[CLS]] [it] ['] s a pleasure to see se [##in] ##feld grip ##ing [about] [the] [bi] ##z [with] [buddies] [chris] [rock] [,] [garry] [shan] ##dling [and] colin quinn [.] [[SEP]] 
INFO:root:test acc: 0.2800 test anti acc: 0.2600
INFO:root:test sparsity: 0.3405 test continuity: 0.1833
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] [steve] [irwin] ['] s method [is] ernest hem ##ming ##way at [accelerated] [speed] [and] volume [.] [[SEP]] 
INFO:root:test acc: 0.2600 test anti acc: 0.2800
INFO:root:test sparsity: 0.3150 test continuity: 0.1997
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] we never really feel involved [with] the story , [as] all of its [ideas] [remain] [just] [that] [:] [abstract] [ideas] [.] [[SEP]] 
INFO:root:test acc: 0.2600 test anti acc: 0.2600
INFO:root:test sparsity: 0.2889 test continuity: 0.1976
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] jason x has che [##es] ##y effects and a ho ##ary plot , but its [mac] [##ab] [##re] [,] [self] [-] [de] ##pre [##cating] [sense] [of] [humor] makes [up] [for] [a] [lot] . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.2800
INFO:root:test sparsity: 0.2652 test continuity: 0.1900
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] but [what] saves [lives] on [the] freeway does not necessarily make for [per] ##su [##asi] [##ve] [viewing] [.] [[SEP]] 
INFO:root:test acc: 0.2600 test anti acc: 0.2600
INFO:root:test sparsity: 0.2353 test continuity: 0.1884
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] [those] [who] [managed] [to] [avoid] the deco ##nst ##ru ##ction ##ist theo ##riz [##ing] [of] [french] [philosopher] [jacques] der ##rid ##a [in] college can now take [an] [85] - minute [brush] - [up] course with the documentary der ##rid ##a . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.3000
INFO:root:test sparsity: 0.2117 test continuity: 0.1808
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] even [as] lame horror flick ##s go , this is lame . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.3400
INFO:root:test sparsity: 0.1807 test continuity: 0.1531
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the film would work much better as a video installation in a museum , where viewers [would] [be] free to [leave] [.] [[SEP]] 
INFO:root:test acc: 0.2800 test anti acc: 0.3000
INFO:root:test sparsity: 0.1632 test continuity: 0.1438
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] finally , a genre movie that delivers - - in a couple of [genres] , no less . [[SEP]] 
INFO:root:test acc: 0.3000 test anti acc: 0.3200
INFO:root:test sparsity: 0.1504 test continuity: 0.1364
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the [premise] for this [ke] ##gger comedy probably sounded brilliant four six - packs and a pitcher [of] [margarita] ##s in , but the film must have [been] written . . . in the th ##ral ##l of a [vicious] [hang] [##over] [.] [[SEP]] 
INFO:root:test acc: 0.2800 test anti acc: 0.2800
INFO:root:test sparsity: 0.1392 test continuity: 0.1411
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] as conceived by mr . sc ##hae ##ffer , christopher and grace [are] little more [than] [collections] [of] qui ##rky traits [lifted] [from] a [screenwriter] ' s [outline] [and] thrown at actors charged with the impossible task of making them [je] ##ll . [[SEP]] 
INFO:root:test acc: 0.3400 test anti acc: 0.3400
INFO:root:test sparsity: 0.1201 test continuity: 0.1197
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] we never really feel involved with the story , as all of its [ideas] [remain] [just] [that] [:] abstract [ideas] [.] [[SEP]] 
INFO:root:test acc: 0.3000 test anti acc: 0.4000
INFO:root:test sparsity: 0.1046 test continuity: 0.1110
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the film would work much better as a video installation in a museum , where viewers [would] [be] free to leave [.] [[SEP]] 
INFO:root:test acc: 0.4000 test anti acc: 0.3200
INFO:root:test sparsity: 0.0899 test continuity: 0.1003
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] the low - [budget] [full] [frontal] was one of the year ' s mu [##rk] [##iest] , intentionally obscure and self - ind ##ul ##gent [pictures] , and [solar] [##is] is its big - budget brother . [SEP] 
INFO:root:test acc: 0.3600 test anti acc: 0.3200
INFO:root:test sparsity: 0.0828 test continuity: 0.0959
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] the low - [budget] [full] [frontal] was one of the year ' s mu [##rk] [##iest] , intentionally obscure and self - ind ##ul ##gent pictures , and [solar] [##is] is its big - budget brother . [SEP] 
INFO:root:test acc: 0.3800 test anti acc: 0.3400
INFO:root:test sparsity: 0.0740 test continuity: 0.0861
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] finally , a genre movie that delivers - - in a couple of genres , no less . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.3400
INFO:root:test sparsity: 0.0685 test continuity: 0.0816
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] cu ##lk ##in [ex] [##udes] none of the charm or char ##ism ##a that might [keep] a more general audience even vaguely interested in his [brat] ##ty character . [SEP] 
INFO:root:test acc: 0.2800 test anti acc: 0.3600
INFO:root:test sparsity: 0.0574 test continuity: 0.0662
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] a go ##b of drive ##l so sick ##ly sweet , even the eager consumers of [moore] ['] [s] [paste] [##urized] [di] [##tti] [##es] will re ##tch [it] [up] [like] [ran] [##ci] ##d cr ##eme [br] [##ule] ##e . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.3800
INFO:root:test sparsity: 0.0524 test continuity: 0.0603
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] if this holiday movie is supposed to be a gift , somebody un [##wr] [##app] [##ed] [it] [early] , [took] [out] all the good stuff , and left behind the crap - l ##rb - literally - rr ##b - . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.4200
INFO:root:test sparsity: 0.0469 test continuity: 0.0553
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the low - budget [full] [frontal] was one of the year ' s mu [##rk] [##iest] , intentionally obscure and self - ind ##ul ##gent pictures , and [solar] [##is] is its big - budget brother . [SEP] 
INFO:root:test acc: 0.3600 test anti acc: 0.4000
INFO:root:test sparsity: 0.0444 test continuity: 0.0524
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] the premise for this ke ##gger comedy probably sounded brilliant four six - packs and a pitcher of [margarita] ##s in , but the film must have [been] written . . . in the th ##ral ##l of a vicious [hang] [##over] . [SEP] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.3400, test acc: 0.2600
INFO:root:train acc: 0.3800, test acc: 0.2600
INFO:root:train acc: 0.3400, test acc: 0.2600
INFO:root:train acc: 0.3400, test acc: 0.2800
INFO:root:train acc: 0.4200, test acc: 0.5000
INFO:root:train acc: 0.5400, test acc: 0.5000
INFO:root:train acc: 0.5400, test acc: 0.4600
INFO:root:train acc: 0.5200, test acc: 0.4400
INFO:root:train acc: 0.5600, test acc: 0.4800
INFO:root:train acc: 0.5600, test acc: 0.4400
INFO:root:test acc: 0.2400 test anti acc: 0.2600
INFO:root:test sparsity: 0.2551 test continuity: 0.2019
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] gangs of new york is an una [##pol] ##oge [##tic] mess , [whose] [only] saving grace [is] [that] [it] [ends] [by] [blowing] just about everything up [.] [[SEP]] 
INFO:root:test acc: 0.3600 test anti acc: 0.2600
INFO:root:test sparsity: 0.2268 test continuity: 0.1917
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] cu ##lk ##in ex ##udes [none] of [the] charm or char ##ism ##a [that] might keep a more general audience [even] vaguely interested in his brat ##ty [character] [.] [[SEP]] 
INFO:root:test acc: 0.3200 test anti acc: 0.2600
INFO:root:test sparsity: 0.1903 test continuity: 0.1773
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] when a set of pre - [shooting] [guidelines] a director came [up] with for his actors turns out to be clever ##er , [better] [written] and of considerable more [interest] than [the] [finished] [film] [,] [that] ['] s [a] [bad] [sign] [.] [[SEP]] 
INFO:root:test acc: 0.2600 test anti acc: 0.2600
INFO:root:test sparsity: 0.1597 test continuity: 0.1473
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] the passions aroused [by] [the] [disco] [##rd] [between] old and [new] cultures are set against [the] strange , stark [beauty] of the mid ##ea [##st] desert , so [loving] ##ly and per ##ceptive ##ly [filmed] [that] [you] [can] [almost] [taste] [the] [des] ##ic [##cated] [air] [.] [[SEP]] 
INFO:root:test acc: 0.2200 test anti acc: 0.2400
INFO:root:test sparsity: 0.1438 test continuity: 0.1316
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[CLS] nicely serves as an examination of a society in transition . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.2600
INFO:root:test sparsity: 0.1284 test continuity: 0.1114
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] of ##t - described as the anti [##dote] to american pie - type sex comedies , it actually has a bundle [in] [common] with [them] , as [the] [film] [diffuse] [##s] [every] [opportunity] [for] [a] breakthrough [[SEP]] 
INFO:root:test acc: 0.2400 test anti acc: 0.4000
INFO:root:test sparsity: 0.1180 test continuity: 0.1005
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] not a cozy or ing ##rat ##iating work , but it ' s challenging , sometimes clever , and [always] interesting , and those are reasons enough to see it . [[SEP]] 
INFO:root:test acc: 0.1800 test anti acc: 0.4000
INFO:root:test sparsity: 0.1080 test continuity: 0.0960
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] acting , particularly by tam ##bor , almost makes ` ` never [again] ' ['] [worth] ##while , but - [l] ##rb - [writer] \ / director - rr ##b - sc ##hae ##ffer [should] [follow] [his] [titular] advice [SEP] 
INFO:root:test acc: 0.2400 test anti acc: 0.4200
INFO:root:test sparsity: 0.1037 test continuity: 0.0950
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] jason x has che ##es ##y [effects] and [a] ho ##ary plot , but its mac ##ab ##re , self - de [##pre] [##cating] sense of humor makes [up] for [a] [lot] . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.4000
INFO:root:test sparsity: 0.0987 test continuity: 0.0926
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] but what saves lives on the freeway does not necessarily make for per ##su ##asi ##ve viewing . [SEP] 
INFO:root:test acc: 0.3200 test anti acc: 0.3000
INFO:root:test sparsity: 0.0931 test continuity: 0.0849
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] gangs of new york is an una [##pol] ##oge ##tic mess , whose only saving grace is [that] [it] [ends] [by] blowing just about everything up . [SEP] 
INFO:root:test acc: 0.2600 test anti acc: 0.2600
INFO:root:test sparsity: 0.0816 test continuity: 0.0792
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] if your senses have n ' t been dull [##ed] [by] slash ##er films and gore ##fest [##s] , if [you] ' re a con ##no ##isse ##ur of psychological horror [,] [this] [is] [your] [ticket] . [SEP] 
INFO:root:test acc: 0.3000 test anti acc: 0.2600
INFO:root:test sparsity: 0.0684 test continuity: 0.0687
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] even as lame horror flick [##s] [go] , this is lame . [SEP] 
INFO:root:test acc: 0.2000 test anti acc: 0.2600
INFO:root:test sparsity: 0.0656 test continuity: 0.0703
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] no movement , no yu ##ks , [not] [much] of anything . [SEP] 
INFO:root:test acc: 0.2000 test anti acc: 0.2600
INFO:root:test sparsity: 0.0617 test continuity: 0.0675
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] no movement , no yu ##ks , [not] [much] of anything . [SEP] 
INFO:root:test acc: 0.1800 test anti acc: 0.2600
INFO:root:test sparsity: 0.0607 test continuity: 0.0665
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] acting , particularly by tam ##bor , almost makes ` ` never [again] ' ' [worth] ##while , but - [l] ##rb - [writer] \ / director - rr ##b - sc ##hae ##ffer [should] [follow] [his] titular advice [SEP] 
INFO:root:test acc: 0.2200 test anti acc: 0.2600
INFO:root:test sparsity: 0.0560 test continuity: 0.0628
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] jason x has che ##es ##y effects and a ho ##ary plot , but its mac ##ab ##re , self - de [##pre] [##cating] sense of humor makes [up] for a [lot] . [SEP] 
INFO:root:test acc: 0.2200 test anti acc: 0.2400
INFO:root:test sparsity: 0.0513 test continuity: 0.0592
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] if your senses have n ' t been dull ##ed [by] slash ##er films and gore ##fest [##s] , if [you] ' re a con ##no ##isse ##ur of psychological horror , [this] [is] [your] [ticket] . [SEP] 
INFO:root:test acc: 0.2400 test anti acc: 0.2400
INFO:root:test sparsity: 0.0469 test continuity: 0.0532
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] as conceived by mr . sc ##hae ##ffer , christopher and grace are little more than collections of qui ##rky traits lifted from a screenwriter ' s outline and thrown at actors [charged] with [the] [impossible] [task] [of] [making] [them] je ##ll . [SEP] 
INFO:root:test acc: 0.3200 test anti acc: 0.3000
INFO:root:test sparsity: 0.0443 test continuity: 0.0503
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] take care of my cat offers a refreshing ##ly [different] slice of asian cinema . [SEP] 
INFO:root:test acc: 0.3400 test anti acc: 0.3000
INFO:root:test sparsity: 0.0424 test continuity: 0.0493
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] as conceived by mr . sc ##hae ##ffer , christopher and grace are little more than collections of qui ##rky traits lifted from a screenwriter ' s outline and thrown at actors [charged] with [the] [impossible] [task] [of] [making] [them] je ##ll . [SEP] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.3000, test acc: 0.4800
INFO:root:train acc: 0.4000, test acc: 0.5000
INFO:root:train acc: 0.3800, test acc: 0.6100
INFO:root:train acc: 0.4800, test acc: 0.6100
INFO:root:train acc: 0.5200, test acc: 0.4900
INFO:root:train acc: 0.4800, test acc: 0.4900
INFO:root:train acc: 0.4600, test acc: 0.5800
INFO:root:train acc: 0.5600, test acc: 0.6500
INFO:root:train acc: 0.6000, test acc: 0.7100
INFO:root:train acc: 0.6200, test acc: 0.6400
INFO:root:test acc: 0.5800 test anti acc: 0.4900
INFO:root:test sparsity: 0.1281 test continuity: 0.1465
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a harmless and [mildly] amusing [family] comedy . [SEP] 
INFO:root:test acc: 0.5400 test anti acc: 0.4900
INFO:root:test sparsity: 0.1186 test continuity: 0.1402
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] not ` terrible filmmaking ' bad , but more like , ' i once had a nightmare like this , and it ' s [now] [coming] [true] ' bad . [SEP] 
INFO:root:test acc: 0.5400 test anti acc: 0.4900
INFO:root:test sparsity: 0.1101 test continuity: 0.1327
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a harmless and [mildly] amusing [family] comedy . [SEP] 
INFO:root:test acc: 0.5000 test anti acc: 0.5000
INFO:root:test sparsity: 0.0978 test continuity: 0.1179
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] the film runs on equal parts of innocence and wisdom - - [wisdom] [that] comes with experience . [SEP] 
INFO:root:test acc: 0.5100 test anti acc: 0.5000
INFO:root:test sparsity: 0.0945 test continuity: 0.1141
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] george , hire a real director and good writers for the next installment , please . [SEP] 
INFO:root:test acc: 0.4900 test anti acc: 0.5000
INFO:root:test sparsity: 0.0886 test continuity: 0.1071
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] a tu ##rg ##id little history lesson , humour ##less and dull . [SEP] 
INFO:root:test acc: 0.5000 test anti acc: 0.5100
INFO:root:test sparsity: 0.0850 test continuity: 0.1050
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] reinforce ##s the often forgotten fact of the world ' s [remarkably] varying [human] population and minds ##et , and its capacity to heal using creative , natural and ancient anti ##dote ##s . [SEP] 
INFO:root:test acc: 0.5600 test anti acc: 0.5100
INFO:root:test sparsity: 0.0833 test continuity: 0.1016
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] for all its alleged youthful fire , xx [##x] is no [less] sub ##ser ##vie ##nt to bond ' s tired formula of [guns] , [girls] [and] ga [##dgets] [while] [brand] ##ishing a new action hero . [SEP] 
INFO:root:test acc: 0.5300 test anti acc: 0.4900
INFO:root:test sparsity: 0.0809 test continuity: 0.0987
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] all these developments and challenges facing santa weigh down the plot [so] heavily that they drain all the film of its energy and [needles] ##sl ##y [strain] [credibility] . [SEP] 
INFO:root:test acc: 0.4800 test anti acc: 0.5100
INFO:root:test sparsity: 0.0787 test continuity: 0.0953
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] a movie that , rather than skip along the seine , more or less sl ##og ##s its way through so ##ggy [paris] , [tongue] [uncomfortably] [in] [cheek] . [SEP] 
INFO:root:test acc: 0.4700 test anti acc: 0.5100
INFO:root:test sparsity: 0.0758 test continuity: 0.0930
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[CLS] this cu ##dd ##ly sequel to the 1999 hit is a little more visually polished , a little fun ##nier , and a little [more] [mad] [##cap] . [SEP] 
INFO:root:test acc: 0.5000 test anti acc: 0.5100
INFO:root:test sparsity: 0.0737 test continuity: 0.0912
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] everything you loved about it in 1982 is still there , for everybody who wants to be a kid again , or show it to [their] [own] [kids] [.] [SEP] 
INFO:root:test acc: 0.5600 test anti acc: 0.5300
INFO:root:test sparsity: 0.0719 test continuity: 0.0888
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[CLS] ser ##ry does a fine job of capturing the climate of the times and , perhaps un ##wi ##tting ##ly , relating it to what is [happening] in america in [2002] . [SEP] 
INFO:root:test acc: 0.5500 test anti acc: 0.5400
INFO:root:test sparsity: 0.0675 test continuity: 0.0830
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[CLS] as lo - fi as the special effects are , the [folks] [who] cobb ##led nemesis [together] ind ##ul ##ge the force of humanity [over] hardware in a [way] [that] [george] [lucas] [has] [long] [forgotten] . [SEP] 
INFO:root:test acc: 0.5200 test anti acc: 0.5300
INFO:root:test sparsity: 0.0616 test continuity: 0.0785
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] not ` terrible filmmaking ' bad , but more like , ' i once had a nightmare like this , and it ' s [now] [coming] true ' bad . [SEP] 
INFO:root:test acc: 0.5100 test anti acc: 0.5300
INFO:root:test sparsity: 0.0585 test continuity: 0.0744
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] a well acted and well intention ##ed s ##no ##oz ##er . [SEP] 
INFO:root:test acc: 0.5300 test anti acc: 0.5200
INFO:root:test sparsity: 0.0551 test continuity: 0.0688
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] a tu ##rg ##id little history lesson , humour ##less and dull . [SEP] 
INFO:root:test acc: 0.5600 test anti acc: 0.5200
INFO:root:test sparsity: 0.0536 test continuity: 0.0664
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] tad ##pole is a sophisticated , funny and good - nature ##d treat , slight but a pleasure . [SEP] 
INFO:root:test acc: 0.5600 test anti acc: 0.5000
INFO:root:test sparsity: 0.0519 test continuity: 0.0636
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] ser ##ry does a fine job of capturing the climate of the times and , perhaps un ##wi ##tting ##ly , relating it to what is [happening] in america in 2002 . [SEP] 
INFO:root:test acc: 0.5900 test anti acc: 0.4900
INFO:root:test sparsity: 0.0504 test continuity: 0.0612
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] the cartoon that is n ' t really good enough to be on afternoon tv is now a movie that is n ' t really good [enough] to be in [theaters] . [SEP] 
INFO:root:test acc: 0.5600 test anti acc: 0.5000
INFO:root:test sparsity: 0.0473 test continuity: 0.0571
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] it is ridiculous , of course . . . but it is also refreshing , di ##sar ##ming , and just [outright] [enjoyable] [despite] [its] [ridiculous] [##ness] . [SEP] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.8458, test acc: 0.8797
INFO:root:train acc: 0.8848, test acc: 0.8792
INFO:root:train acc: 0.9052, test acc: 0.8825
INFO:root:train acc: 0.9165, test acc: 0.8501
INFO:root:train acc: 0.9374, test acc: 0.8759
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.8306, test acc: 0.8748
INFO:root:train acc: 0.8867, test acc: 0.8792
INFO:root:test acc: 0.5074 test anti acc: 0.7716
INFO:root:test sparsity: 0.0510 test continuity: 0.0931
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a beautiful and haunting examination of the stories we tell ourselves to make sense of the mundane horrors of the world . [[SEP]] 
INFO:root:test acc: 0.5349 test anti acc: 0.8144
INFO:root:test sparsity: 0.0990 test continuity: 0.1017
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] in his debut as a film director , den ##zel washington delivers a lean and engaging work [.] [[SEP]] 
INFO:root:test acc: 0.5036 test anti acc: 0.8166
INFO:root:test sparsity: 0.0623 test continuity: 0.1029
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] if shay ##ama ##lan wanted to tell a story about a man who loses his faith , why did n ' t he just do it , instead of using bad sci - fi as window [dressing] [?] [[SEP]] 
INFO:root:test acc: 0.5728 test anti acc: 0.7853
INFO:root:test sparsity: 0.1443 test continuity: 0.1601
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [imagine] a really bad community theater production of west side story without the songs . [[SEP]] 
INFO:root:test acc: 0.5876 test anti acc: 0.7952
INFO:root:test sparsity: 0.1429 test continuity: 0.1735
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [if] [shay] ##ama ##lan wanted to tell a story about a man who loses his faith , why did n ' t he just do it , instead of using bad sci - fi as window [dressing] [?] [[SEP]] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.8431, test acc: 0.8688
INFO:root:train acc: 0.8850, test acc: 0.8770
INFO:root:test acc: 0.6414 test anti acc: 0.6172
INFO:root:test sparsity: 0.2698 test continuity: 0.2085
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [consists] of a plot and jokes done too often by people far [more] [talented] than ali [g] [[SEP]] 
INFO:root:test acc: 0.7221 test anti acc: 0.6645
INFO:root:test sparsity: 0.3434 test continuity: 0.2604
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [if] [shay] [##ama] ##lan [wanted] to [tell] a story about a man who loses his faith , why did n ' t he just do it , instead of using bad sci - fi [as] [window] [dressing] [?] [[SEP]] 
INFO:root:test acc: 0.7205 test anti acc: 0.6722
INFO:root:test sparsity: 0.3292 test continuity: 0.2330
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [earnest] [falls] short of its ideal predecessor largely due to parker ' s ill - advised med ##dling with the timeless [source] [material] [.] [[SEP]] 
INFO:root:test acc: 0.6886 test anti acc: 0.7150
INFO:root:test sparsity: 0.2521 test continuity: 0.1751
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] [uneasy] [mis] [##hma] ##sh of styles and [genres] [.] [[SEP]] 
INFO:root:test acc: 0.7540 test anti acc: 0.6661
INFO:root:test sparsity: 0.3034 test continuity: 0.2090
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [uneasy] [mis] [##hma] ##sh of [styles] and [genres] [.] [[SEP]] 
INFO:root:test acc: 0.7655 test anti acc: 0.6744
INFO:root:test sparsity: 0.3301 test continuity: 0.2361
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] with its hints of a greater [intelligence] [lurking] somewhere , the ring makes its [stupidity] [more] [than] [obvious] [.] [[SEP]] 
INFO:root:test acc: 0.7902 test anti acc: 0.6590
INFO:root:test sparsity: 0.3988 test continuity: 0.2926
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] smith profiles five extraordinary american homes , and because the [owners] [seem] [fully] [aware] of the uses and [abuses] of fame , it ' s a [pleasure] to [enjoy] their [eccentric] [##ities] [.] [[SEP]] 
INFO:root:test acc: 0.7858 test anti acc: 0.6227
INFO:root:test sparsity: 0.4151 test continuity: 0.3143
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] in his debut as a film director , den [##zel] [washington] [delivers] a [lean] and [engaging] [work] [.] [[SEP]] 
INFO:root:test acc: 0.7957 test anti acc: 0.6200
INFO:root:test sparsity: 0.4303 test continuity: 0.3379
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] in his debut as a film director , den [##zel] [washington] [delivers] a [lean] and [engaging] [work] [.] [[SEP]] 
INFO:root:test acc: 0.8199 test anti acc: 0.6041
INFO:root:test sparsity: 0.4267 test continuity: 0.4671
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [my] [response] to the film is [best] [described] as [luke] [##war] ##m . [[SEP]] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.8342, test acc: 0.8611
INFO:root:train acc: 0.8860, test acc: 0.8781
INFO:root:test acc: 0.5783 test anti acc: 0.7485
INFO:root:test sparsity: 0.1696 test continuity: 0.1289
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] tailored to [entertain] [!] [[SEP]] 
INFO:root:test acc: 0.7073 test anti acc: 0.7452
INFO:root:test sparsity: 0.2681 test continuity: 0.2075
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a beautiful and haunting examination of the stories we tell [ourselves] to make [sense] of the mundane [horrors] of the [world] [.] [[SEP]] 
INFO:root:test acc: 0.7435 test anti acc: 0.7227
INFO:root:test sparsity: 0.3010 test continuity: 0.2447
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] [tailored] to [entertain] [!] [[SEP]] 
INFO:root:test acc: 0.7584 test anti acc: 0.6804
INFO:root:test sparsity: 0.3400 test continuity: 0.2804
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] [consists] of a plot and [jokes] done too often by people far [more] [talented] [than] [ali] [g] [[SEP]] 
INFO:root:test acc: 0.7716 test anti acc: 0.6848
INFO:root:test sparsity: 0.3369 test continuity: 0.2988
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [uneasy] [mis] [##hma] ##sh of styles and [genres] [.] [[SEP]] 
INFO:root:test acc: 0.8012 test anti acc: 0.6271
INFO:root:test sparsity: 0.4423 test continuity: 0.3536
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] the way [cop] [##pol] ##a [prof] [##esses] his love for movies - - both [colorful] [pop] [junk] and the [classics] that [une] [##qui] [##vo] [##cal] [##ly] [qualify] as art - - is [gi] [##dd] [##ily] [entertaining] [.] [[SEP]] 
INFO:root:test acc: 0.7957 test anti acc: 0.6249
INFO:root:test sparsity: 0.4699 test continuity: 0.3813
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] my [response] to the film is [best] described as luke [##war] [##m] [.] [[SEP]] 
INFO:root:test acc: 0.8160 test anti acc: 0.6090
INFO:root:test sparsity: 0.4534 test continuity: 0.3822
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] he ' d [create] a [movie] [better] than this . [[SEP]] 
INFO:root:test acc: 0.8215 test anti acc: 0.5530
INFO:root:test sparsity: 0.5014 test continuity: 0.4077
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] the [story] [loses] its [bite] in a last - [minute] [happy] [ending] that ' s [even] [less] [plausible] than the rest of the [picture] [.] [[SEP]] 
INFO:root:test acc: 0.8171 test anti acc: 0.6282
INFO:root:test sparsity: 0.3999 test continuity: 0.4510
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [imagine] a [really] [bad] [community] theater production of west side story [without] the [songs] . [[SEP]] 
INFO:root:test acc: 0.8298 test anti acc: 0.6161
INFO:root:test sparsity: 0.4416 test continuity: 0.4832
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] [fans] of the [animated] [wildlife] [adventure] show will be in war ##th ##og [heaven] ; [others] [need] [not] [necessarily] [apply] . [[SEP]] 
INFO:root:test acc: 0.8166 test anti acc: 0.6123
INFO:root:test sparsity: 0.4469 test continuity: 0.4923
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] [th] [##ur] ##man and lewis are [hilarious] [throughout] . [[SEP]] 
INFO:root:test acc: 0.8243 test anti acc: 0.5865
INFO:root:test sparsity: 0.4583 test continuity: 0.4970
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] the [way] [cop] [##pol] ##a [prof] [##esses] his [love] for movies - - both [colorful] [pop] [junk] and the [classics] that [une] [##qui] [##vo] [##cal] ##ly [qualify] as art - - is [gi] [##dd] [##ily] [entertaining] . [[SEP]] 
INFO:root:test acc: 0.8100 test anti acc: 0.5596
INFO:root:test sparsity: 0.4254 test continuity: 0.4845
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] it is [interesting] and [fun] to see [good] [##all] and her [chi] ##mp ##an ##zee ##s on the bigger - than - life screen . [[SEP]] 
INFO:root:test acc: 0.8243 test anti acc: 0.5634
INFO:root:test sparsity: 0.4478 test continuity: 0.5136
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] [fans] of the [animated] [wildlife] [adventure] show will be in [war] [##th] [##og] [heaven] ; [others] [need] [not] [necessarily] [apply] . [[SEP]] 
INFO:root:test acc: 0.8100 test anti acc: 0.6107
INFO:root:test sparsity: 0.4355 test continuity: 0.5148
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] . . . a [haunting] [vision] , with [images] that [seem] [more] [like] [disturbing] [hall] [##uc] [##inations] . [[SEP]] 
INFO:root:test acc: 0.8177 test anti acc: 0.5969
INFO:root:test sparsity: 0.4301 test continuity: 0.5106
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] . . . a [haunting] [vision] , with [images] that [seem] [more] [like] [disturbing] [hall] ##uc [##inations] . [[SEP]] 
INFO:root:test acc: 0.8292 test anti acc: 0.5398
INFO:root:test sparsity: 0.5196 test continuity: 0.5441
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [my] [response] to the film is [best] [described] as [luke] [##war] ##m . [[SEP]] 
INFO:root:test acc: 0.8298 test anti acc: 0.5986
INFO:root:test sparsity: 0.4721 test continuity: 0.5236
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] [bran] [##agh] , in his most [forceful] [non] - [shakespeare] [screen] [performance] , [grounds] [even] the [soft] [##est] [moments] in the [angry] [revolt] of his [wit] . [[SEP]] 
INFO:root:test acc: 0.8237 test anti acc: 0.5903
INFO:root:test sparsity: 0.4743 test continuity: 0.5328
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] in his [debut] as a film director , den [##zel] [washington] [delivers] a [lean] and [engaging] [work] . [[SEP]] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.8396, test acc: 0.8742
INFO:root:train acc: 0.8844, test acc: 0.8764
INFO:root:test acc: 0.4948 test anti acc: 0.7573
INFO:root:test sparsity: 0.0497 test continuity: 0.0969
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] if shay ##ama ##lan wanted to tell a story about a man who loses his faith , why did n ' t he just do it , instead of using bad sci - fi as window dressing ? [[SEP]] 
INFO:root:test acc: 0.5189 test anti acc: 0.7639
INFO:root:test sparsity: 0.1320 test continuity: 0.0995
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] if shay ##ama ##lan wanted to tell a story about a man who loses his faith , why did n ' t he just do it , instead of using bad sci - fi as window [dressing] [?] [[SEP]] 
INFO:root:test acc: 0.5167 test anti acc: 0.8331
INFO:root:test sparsity: 0.1056 test continuity: 0.1083
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a bree ##zy , divert ##ing , conventional , well - acted tale of two men locked in an ongoing game of cat - and - cat [.] [[SEP]] 
INFO:root:test acc: 0.7611 test anti acc: 0.8572
INFO:root:test sparsity: 0.0688 test continuity: 0.0612
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] he ' d create a movie better than this . [[SEP]] 
INFO:root:test acc: 0.8649 test anti acc: 0.8874
INFO:root:test sparsity: 0.0546 test continuity: 0.0503
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the period - - swinging london in the time of the mod ##s and the rocker ##s - - gets the once - over once again in gangster no . 1 , but falls apart long before the end . [[SEP]] 
INFO:root:test acc: 0.8556 test anti acc: 0.8704
INFO:root:test sparsity: 0.0517 test continuity: 0.0524
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] fans of the animated wildlife adventure show will be in war ##th ##og heaven ; others need not necessarily apply . [[SEP]] 
INFO:root:test acc: 0.8523 test anti acc: 0.8649
INFO:root:test sparsity: 0.0531 test continuity: 0.0569
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] diane lane shine ##s in un ##fa ##ith ##ful . [[SEP]] 
INFO:root:test acc: 0.8731 test anti acc: 0.8770
INFO:root:test sparsity: 0.0523 test continuity: 0.0646
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] an after ##sch ##ool special without the courage of its convictions . [SEP] 
INFO:root:test acc: 0.8556 test anti acc: 0.8753
INFO:root:test sparsity: 0.0510 test continuity: 0.0683
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] the premise is in extremely bad taste , and the film ' s supposed insights are so poorly thought - out and substance - free that even a high school senior taking his or her first psychology class could dismiss them . [SEP] 
INFO:root:test acc: 0.8759 test anti acc: 0.8770
INFO:root:test sparsity: 0.0485 test continuity: 0.0698
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a dr ##oll , well - acted , character - driven comedy with unexpected deposits of feeling . [[SEP]] 
INFO:root:test acc: 0.8726 test anti acc: 0.8753
INFO:root:test sparsity: 0.0626 test continuity: 0.0690
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] an after ##sch ##ool special without the courage of its convictions . [SEP] 
INFO:root:test acc: 0.8808 test anti acc: 0.8814
INFO:root:test sparsity: 0.0700 test continuity: 0.0716
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [earnest] falls short of its ideal predecessor largely due to parker ' s ill - advised med ##dling with the timeless source material . [SEP] 
INFO:root:test acc: 0.8731 test anti acc: 0.8737
INFO:root:test sparsity: 0.0894 test continuity: 0.0745
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [imagine] a really bad community theater production of west side story without the songs . [SEP] 
INFO:root:test acc: 0.8715 test anti acc: 0.8731
INFO:root:test sparsity: 0.0880 test continuity: 0.0740
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] . . . a haunting vision , with images that seem more like disturbing hall ##uc ##inations . [[SEP]] 
INFO:root:test acc: 0.8616 test anti acc: 0.8627
INFO:root:test sparsity: 0.1018 test continuity: 0.0764
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] a dr ##oll , well - acted , character - driven comedy with unexpected deposits of [feeling] [.] [[SEP]] 
INFO:root:test acc: 0.8720 test anti acc: 0.8720
INFO:root:test sparsity: 0.0964 test continuity: 0.0753
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [extreme] o ##ops - o ##ops , ops , no matter how you spell it , it ' s still a mistake to go see it . [SEP] 
INFO:root:test acc: 0.8710 test anti acc: 0.8693
INFO:root:test sparsity: 0.0991 test continuity: 0.0771
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] the scope of the si ##lb ##ers ##tein family is large and we grow attached to their lives , full of strength , warmth and vital ##ity . [.] [[SEP]] 
INFO:root:test acc: 0.8583 test anti acc: 0.8594
INFO:root:test sparsity: 0.1030 test continuity: 0.0770
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [extreme] o ##ops - o ##ops , ops , no matter how you spell it , it ' s still a mistake to go see it . [[SEP]] 
INFO:root:test acc: 0.8775 test anti acc: 0.8781
INFO:root:test sparsity: 0.1031 test continuity: 0.0745
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] diane lane shine ##s in un ##fa ##ith ##ful [.] [[SEP]] 
INFO:root:test acc: 0.8770 test anti acc: 0.8803
INFO:root:test sparsity: 0.1048 test continuity: 0.0767
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [uneasy] mis ##hma ##sh of styles and genres . [SEP] 
INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/abishkar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/abishkar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:transformers.configuration_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": true,
  "output_hidden_states": true,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/abishkar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:root:train acc: 0.8418, test acc: 0.8550
INFO:root:train acc: 0.8857, test acc: 0.8830
INFO:root:test acc: 0.5810 test anti acc: 0.7128
INFO:root:test sparsity: 0.1599 test continuity: 0.1122
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] uneasy mis ##hma ##sh of styles and genres [.] [[SEP]] 
INFO:root:test acc: 0.5997 test anti acc: 0.7864
INFO:root:test sparsity: 0.1689 test continuity: 0.1150
INFO:root:Gold Label: 1 Pred label: 0
INFO:root:[CLS] tailored to entertain [!] [[SEP]] 
INFO:root:test acc: 0.5777 test anti acc: 0.7814
INFO:root:test sparsity: 0.1398 test continuity: 0.1182
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[CLS] the screenplay sabotage ##s the movie ' s strengths at almost every [jun] [##cture] [.] [[SEP]] 
INFO:root:test acc: 0.6129 test anti acc: 0.7853
INFO:root:test sparsity: 0.1176 test continuity: 0.1796
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[CLS] th ##ur ##man and lewis are [hilarious] throughout . [[SEP]] 
INFO:root:test acc: 0.7073 test anti acc: 0.6941
INFO:root:test sparsity: 0.2416 test continuity: 0.2502
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [the] story [loses] its bite in a last - minute happy ending that ' s even [less] plausible than the rest of the picture . [[SEP]] 
INFO:root:test acc: 0.7271 test anti acc: 0.6936
INFO:root:test sparsity: 0.2873 test continuity: 0.2714
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] [earnest] [falls] [short] of its ideal predecessor largely due to parker ' s ill - [advised] [med] [##dling] with the timeless source material . [[SEP]] 
INFO:root:test acc: 0.7227 test anti acc: 0.6507
INFO:root:test sparsity: 0.2522 test continuity: 0.2994
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] [consists] of a plot and [jokes] done too often by people far more [talented] [than] [ali] [g] [[SEP]] 
INFO:root:test acc: 0.7490 test anti acc: 0.7106
INFO:root:test sparsity: 0.2547 test continuity: 0.2924
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[CLS] the story [loses] its bite in a last - minute happy ending that ' s [even] [less] [plausible] than the rest of the [picture] . [[SEP]] 
INFO:root:test acc: 0.7721 test anti acc: 0.6738
INFO:root:test sparsity: 0.3180 test continuity: 0.3576
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [extreme] o [##ops] - o ##ops , ops , no matter how you spell it , it ' s [still] a [mistake] to go [see] it [.] [[SEP]] 
INFO:root:test acc: 0.7842 test anti acc: 0.6639
INFO:root:test sparsity: 0.3241 test continuity: 0.3749
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] with its [hints] of a greater intelligence lurking somewhere , the ring makes its [stupidity] [more] than [obvious] . [[SEP]] 
INFO:root:test acc: 0.7990 test anti acc: 0.6255
INFO:root:test sparsity: 0.3660 test continuity: 0.3749
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] the period - - [swinging] london in the time of the mod ##s and the rocker ##s - - gets the once - over once again in [gangster] [no] . 1 , but [falls] [apart] long before the end . [[SEP]] 
INFO:root:test acc: 0.7985 test anti acc: 0.6562
INFO:root:test sparsity: 0.3533 test continuity: 0.3941
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] the period - - [swinging] london in the time of the mod ##s and the rocker ##s - - gets the once - over once again in [gangster] [no] . 1 , but [falls] [apart] long before the end . [[SEP]] 
INFO:root:test acc: 0.7996 test anti acc: 0.6403
INFO:root:test sparsity: 0.3415 test continuity: 0.3869
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] a [bree] [##zy] , [divert] ##ing , conventional , well - acted tale of two men locked in an ongoing game of cat - and - cat . [[SEP]] 
INFO:root:test acc: 0.8083 test anti acc: 0.6398
INFO:root:test sparsity: 0.3480 test continuity: 0.3915
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] . . . a [haunting] vision , with images that [seem] more like [disturbing] hall ##uc [##inations] . [[SEP]] 
INFO:root:test acc: 0.8111 test anti acc: 0.6403
INFO:root:test sparsity: 0.3955 test continuity: 0.4330
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [uneasy] [mis] [##hma] [##sh] of styles and [genres] . [[SEP]] 
INFO:root:test acc: 0.8149 test anti acc: 0.6452
INFO:root:test sparsity: 0.3619 test continuity: 0.4135
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] [th] ##ur ##man and lewis are [hilarious] [throughout] . [[SEP]] 
INFO:root:test acc: 0.8094 test anti acc: 0.6557
INFO:root:test sparsity: 0.3521 test continuity: 0.4149
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] he ' d create a movie [better] than this . [[SEP]] 
INFO:root:test acc: 0.8226 test anti acc: 0.6222
INFO:root:test sparsity: 0.3973 test continuity: 0.4595
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] [earnest] [falls] [short] of its [ideal] [predecessor] [largely] due to parker ' s [ill] - [advised] [med] [##dling] with the [timeless] source material . [[SEP]] 
INFO:root:test acc: 0.8116 test anti acc: 0.6612
INFO:root:test sparsity: 0.3417 test continuity: 0.4150
INFO:root:Gold Label: 0 Pred label: 0
INFO:root:[[CLS]] the period - - swinging london in the time of the mod ##s and the rocker ##s - - gets the once - over once again in [gangster] [no] . 1 , but [falls] [apart] long before the end . [[SEP]] 
INFO:root:test acc: 0.8160 test anti acc: 0.6332
INFO:root:test sparsity: 0.3547 test continuity: 0.4218
INFO:root:Gold Label: 0 Pred label: 1
INFO:root:[[CLS]] . . . a [haunting] vision , with images that [seem] more like [disturbing] hall ##uc [##inations] . [[SEP]] 
INFO:root:test acc: 0.8111 test anti acc: 0.6178
INFO:root:test sparsity: 0.3376 test continuity: 0.4103
INFO:root:Gold Label: 1 Pred label: 1
INFO:root:[[CLS]] a [beautiful] and [haunting] [examination] of the stories we tell ourselves to make sense of the [mundane] [horrors] of the world . [[SEP]] 
